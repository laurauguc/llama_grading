{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurauguc/llama_grading/blob/main/Llama_Grading_Model_Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r44MsoJZJEXh"
      },
      "source": [
        "# Exploring Llama Models\n",
        "\n",
        "## Models Info\n",
        "\n",
        "Llama-3.1-8B-Instruct.\n",
        "\n",
        "Model card: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
        "\n",
        "Llama models GitHub: https://github.com/meta-llama/llama-models\n",
        "\n",
        "Meta release (see performance): https://ai.meta.com/blog/meta-llama-3-1/\n",
        "\n",
        "Selected because multi-lingual but not multi-modal. Smallest of the medium sized varieties. Fits on Colab Pro's GPUs, leaving plenty of memory for fine-tuning.\n",
        "\n",
        "Start with a lightweight model (Llama-3.2-1B-Instruct) for experimentation. See performance here: https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/\n",
        "\n",
        "Performance note:  Llama 3.1 8B outperforms GPT 3.5 Turbo. With fine-tuning can achieve further improvement.\n",
        "\n",
        "Languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n",
        "\n",
        "Also: Deploying LLaMA 3 70B model is much more challenging. No GPU has enough VRAM for this model so you will need to provision a multi-GPU instance (need g5.48xlarge instance on AWS). Much more expensive."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "j43PgsJtHIEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model memory needs\n",
        "\n",
        "\n",
        "| Model                 | Inference Speed | Memory |\n",
        "|-----------------------|------------------------------|----------------|\n",
        "| Llama-3.2-1B-Instruct | 53 tokens/sec                | 5 GB         |\n",
        "| Llama-3.2-3B-Instruct | 32.5 tokens/sec                | 13 GB         |\n",
        "| Llama-3.1-8B-Instruct | 28.5 tokens/sec                | 32.1 GB         |\n",
        "\n",
        "Note: Used NVIDIA A100-SXM4-40GB GPU to estimate inference speeds. Not counting input tokens."
      ],
      "metadata": {
        "id": "USwknG3jGfsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab Pro hardware options\n",
        "\n",
        "| Accelerator     | High-RAM | CPU Cores | GPU Model  | GPU Memory | RAM         | Disk Space | GPU Capability Score |\n",
        "|-----------------|----------|-----------|------------|------------|-------------|------------|----------------------|\n",
        "| **CPU Only**    | No       | 2-4       | None       | N/A        | ~12 GB      | ~100 GB    | N/A                  |\n",
        "| **CPU Only**    | Yes      | 2-4       | None       | N/A        | ~25 GB      | ~150 GB    | N/A                  |\n",
        "| **A100 GPU**    | No       | 8         | NVIDIA A100 | 40 GB     | ~12 GB      | ~100 GB    | 9                    |\n",
        "| **A100 GPU**    | Yes      | 8         | NVIDIA A100 | 40 GB     | ~25 GB      | ~150 GB    | 9                    |\n",
        "| **L4 GPU**      | No       | 4-8       | NVIDIA L4  | 24 GB      | ~12 GB      | ~100 GB    | 8                    |\n",
        "| **L4 GPU**      | Yes      | 4-8       | NVIDIA L4  | 24 GB      | ~25 GB      | ~150 GB    | 8                    |\n",
        "| **T4 GPU**      | No       | 4-8       | NVIDIA T4  | 16 GB      | ~12 GB      | ~100 GB    | 7                    |\n",
        "| **T4 GPU**      | Yes      | 4-8       | NVIDIA T4  | 16 GB      | ~25 GB      | ~150 GB    | 7                    |\n",
        "| **TPU v2-8**    | No       | -         | TPU v2-8   | 64 GB      | ~12 GB      | ~100 GB    | 7.5                  |\n",
        "| **TPU v2-8**    | Yes      | -         | TPU v2-8   | 64 GB      | ~25 GB      | ~150 GB    | 7.5                  |\n",
        "\n",
        "Notes\n",
        "\n",
        "- **RAM**: With High-RAM selected, Colab increases available RAM from around 12 GB to approximately 25 GB or more.\n",
        "- **CPU Cores**: Actual CPU core count can vary depending on availability; typically, Colab Pro allocates between 2-8 cores depending on the selected hardware.\n",
        "- **Disk Space**: Disk space also varies based on High-RAM selection and can fluctuate slightly based on Colab's backend resources.\n",
        "- **GPU Capability Score**: Estimated relative score based on GPU performance for deep learning tasks; the scores are approximations for comparison purposes only.\n",
        "- **TPU v2-8**: TPUs do not use traditional GPUs; instead, they are Tensor Processing Units designed specifically for TensorFlow/PyTorch-based deep learning tasks.\n",
        "\n",
        "The **High-RAM option** generally adds more memory (RAM) and disk space, but does not impact GPU memory or capability score.\n",
        "\n",
        "\n",
        "Changes noticed:\n",
        "\n",
        "* CPU-only High Ram: RAM ~ 50 GB and number of CPU cores 8. Total disk space 220 GB.\n",
        "\n",
        "* A 100 GPU. GPU capabability score: 8. RAM 83 GB. disk space 235 GB."
      ],
      "metadata": {
        "id": "7zyQ6cMqxvKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling Strategy: Use Criterion-Specific LLMs.\n",
        "\n",
        "To streamline fine-tuning, consider building a separate LLM for each individual grading criterion. Since each criterion may appear across multiple rubrics, organizing fine-tuning by criterion rather than by rubric simplifies the process, addressing each assessment dimension independently.\n",
        "\n",
        "Fine-tuning needs vary depending on the complexity of the writing criterion:\n",
        "\n",
        "* **Basic tasks** (e.g., grammar, sentence structure) can be managed by smaller models.\n",
        "* **Context-sensitive tasks** (e.g., audience appropriateness) require medium-sized models.\n",
        "* **High-level assessments** (e.g., cohesiveness, content quality) are best suited for larger, fine-tuned models.\n",
        "\n",
        "Also, justifying assessments can be more complex than generating a score alone. Decide whether to handle justification independently or together with scoring. Need to consider which approach is more flexible to custom grading rubrics.\n",
        "\n",
        "### Writing Assessment Alone\n",
        "\n",
        "| Writing Criteria           | Model Complexity (Parameters)     | Fine-Tuning Required? |\n",
        "|---------------------------------------|-----------------------------------|------------------------|\n",
        "| Grammar                               | Low (100M - 500M)                | No                    |\n",
        "| Citation and Referencing              | Low (100M - 500M)                | No                    |\n",
        "| Adherence to Prompt                   | Low (100M - 500M)                | No                    |\n",
        "| Sentence Structure                    | Medium (500M - 1B)               | No                    |\n",
        "| Language Appropriateness for Audience | Medium (500M - 1B)               | Yes                   |\n",
        "| Vocabulary Usage                      | Medium (500M - 1B)               | Yes                   |\n",
        "| Writing Cohesiveness                  | High (1B - 6B)                   | Yes                   |\n",
        "| Argumentation Quality                 | High (1B - 6B)                   | Yes                   |\n",
        "| Creativity and Originality            | High (1B - 6B)                   | Yes                   |\n",
        "| Content Quality                       | Very High (6B+)                  | Yes                   |\n",
        "\n",
        "### Writing Assessment Justifaction\n",
        "\n",
        "| Writing Criteria                     | Model Complexity (Parameters)     | Fine-Tuning Required? |\n",
        "|--------------------------------------|-----------------------------------|------------------------|\n",
        "| Grammar                              | Medium (500M - 1B)               | No                     |\n",
        "| Citation and Referencing             | Medium (500M - 1B)               | No                     |\n",
        "| Adherence to Prompt                  | Medium (500M - 1B)               | No                     |\n",
        "| Sentence Structure                   | Medium-High (1B - 3B)            | Yes                    |\n",
        "| Language Appropriateness for Audience | High (3B - 6B)                   | Yes                    |\n",
        "| Vocabulary Usage                     | High (3B - 6B)                   | Yes                    |\n",
        "| Writing Cohesiveness                 | Very High (6B+)                   | Yes                    |\n",
        "| Argumentation Quality                | Very High (6B+)                   | Yes                    |\n",
        "| Creativity and Originality           | Very High (6B+)                   | Yes                    |\n",
        "| Content Quality                      | Very High (6B+)                   | Yes                    |\n",
        "\n"
      ],
      "metadata": {
        "id": "aWk3IXNGvwFu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM_6UjzOQbp9"
      },
      "source": [
        "## Steps\n",
        "\n",
        "0. Set-Up\n",
        "\n",
        "1. Prompting\n",
        "\n",
        "2. Fine-tuning: https://www.llama.com/docs/how-to-guides/fine-tuning/\n",
        "\n",
        "3. Orchestrate. Use LangChain to orchestrate LLM calls.\n",
        "\n",
        "4. Deploy: https://nlpcloud.com/how-to-install-and-deploy-llama-3-into-production.html?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQsz3vC2G2Q0"
      },
      "source": [
        "## 0. Set-Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwEwUknyG9xI",
        "outputId": "10e1db2e-e3d1-458d-a01f-88ed429e3d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  0\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "import torch\n",
        "device = 0 if torch.cuda.is_available() else -1 # HF: Users can specify device argument as an integer, -1 meaning “CPU”, >= 0 referring the CUDA device ordinal.\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg9FANT3H0Vh",
        "outputId": "a00b5781-4811-4421-89b3-4a2772af9705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU\n",
            "Total RAM: 83.48 GB\n",
            "Available RAM: 79.91 GB\n",
            "Number of cores: 12\n",
            "\n",
            "GPU\n",
            "Total VRAM: 42.482 GB\n",
            "Available VRAM: 36.881 GB\n",
            "GPU capability score: (8, 0)\n",
            "\n",
            "Disk\n",
            "Total space: 235.68 GB\n",
            "Available space: 201.09 GB\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def computing_info():\n",
        "  #print(f\"PyTorch version: {torch.__version__}\")\n",
        "  #print(f\"CUDA version: {torch.version.cuda}\")\n",
        "  # Get RAM information\n",
        "  mem = psutil.virtual_memory()\n",
        "  print(\"CPU\")\n",
        "  print(f\"Total RAM: {round(mem.total / (1024.0 ** 3), 2)} GB\")\n",
        "  print(f\"Available RAM: {round(mem.available / (1024.0 ** 3), 2)} GB\")\n",
        "  NUM_WORKERS = os.cpu_count()\n",
        "  print(f\"Number of cores: {NUM_WORKERS}\")\n",
        "\n",
        "  # Get GPU information (if available)\n",
        "  if torch.cuda.is_available():\n",
        "    total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n",
        "    print(\"\\nGPU\")\n",
        "    print(f\"Total VRAM: {round(total_gpu_memory * 1e-9, 3)} GB\")\n",
        "    print(f\"Available VRAM: {round(total_free_gpu_memory * 1e-9, 3)} GB\")\n",
        "    GPU_SCORE = torch.cuda.get_device_capability()\n",
        "    print(f\"GPU capability score: {GPU_SCORE}\")\n",
        "  else:\n",
        "    print(\"No GPU available.\")\n",
        "\n",
        "  # Get disk space information\n",
        "  disk = psutil.disk_usage('/')\n",
        "  print(\"\\nDisk\")\n",
        "  print(f\"Total space: {round(disk.total / (1024.0 ** 3), 2)} GB\")\n",
        "  print(f\"Available space: {round(disk.free / (1024.0 ** 3), 2)} GB\")\n",
        "computing_info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM-A_2TyiI31",
        "outputId": "651307e1-a491-4769-aa59-fc1598984940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Oct 28 17:31:55 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0              48W / 400W |   5341MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpCb3lHYG68S"
      },
      "source": [
        "## 1. Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0FcbPXRL1Di"
      },
      "outputs": [],
      "source": [
        "#!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZgyBZ2ELtgH"
      },
      "outputs": [],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model_name,\n",
        "                device = device,\n",
        "                max_new_tokens = 1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "computing_info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoBt44PPa7Wi",
        "outputId": "f2827fa7-81a2-4278-e459-0af194640126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU\n",
            "Total RAM: 83.48 GB\n",
            "Available RAM: 79.25 GB\n",
            "Number of cores: 12\n",
            "\n",
            "GPU\n",
            "Total VRAM: 42.482 GB\n",
            "Available VRAM: 32.001 GB\n",
            "GPU capability score: (8, 0)\n",
            "\n",
            "Disk\n",
            "Total space: 235.68 GB\n",
            "Available space: 201.09 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "user_promts = [\"Who are you?\",\n",
        "               \"Explain why 8 + 8 = 16.\",\n",
        "               \"Write a poem about LLMs.\",\n",
        "               \"Explain the impact of generative AI on corporate carbon goals.\"]\n",
        "outputs = []\n",
        "\n",
        "start_time = time.time()\n",
        "for user_promt in user_promts:\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": user_promt},\n",
        "  ]\n",
        "  output = pipe(messages,\n",
        "                do_sample=False, temperature = None, top_p = None, # by default, temperature = 0.6 and top_p = .9. Need to unset these variables when do_sample = False\n",
        "                pad_token_id = pipe.tokenizer.eos_token_id)\n",
        "  outputs.append(output[0]['generated_text'][1]['content'])\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "n_output_tokens = len(tokenizer.encode(\". \".join(outputs)))\n",
        "\n",
        "# Calculate inference speed\n",
        "elapsed_time = end_time - start_time\n",
        "inference_speed = n_output_tokens / elapsed_time  # tokens per second\n",
        "print(inference_speed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHfkfzHxEc6c",
        "outputId": "924d7efa-f64e-4f1d-f1f8-6e60801b6cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56.36671471830743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-QdudRGfdnfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next\n",
        "\n",
        "* Start tackling small no-fine tuning evaluation tasks.\n",
        "\n",
        "## 2. Fine-Tuning\n",
        "\n",
        "## 3. Orchestration\n",
        "\n",
        "Will likely have a variety of models to use in different circumstances (e.g. a particular grading rubric will require a set of models). Note: need to consider how to handle \"unsupported criteria\" (likely through a proprietary model until supported) and how to handle custom grading rubric that define the the criteria levels differently.\n",
        "\n",
        "## 4. Deployment"
      ],
      "metadata": {
        "id": "ZJT7D1EPdm19"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8mO-rrxudmWn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNss3+rUhtDDvYVjnrdlgNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}